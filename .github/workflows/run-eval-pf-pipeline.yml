name: Test and Evaluate Prompts with Promptflow

on:
  workflow_dispatch:
  push:
    branches: [ main ]

env:
  GROUP: ${{ secrets.GROUP }}
  WORKSPACE: ${{ secrets.WORKSPACE }}
  SUBSCRIPTION: ${{ secrets.SUBSCRIPTION }}

jobs:
  login-run-eval:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout
      uses: actions/checkout@v3

    - name: Azure login
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Set default subscription
      run: az account set -s ${{ env.SUBSCRIPTION }}

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11.4"

    - name: Install Promptflow CLI
      run: pip install -r promptflow/web-classification/requirements.txt

    - name: Run promptflow pipeline
      run: |
        pfazure run create \
          -f promptflow/web-classification/run.yml \
          --subscription ${{ env.SUBSCRIPTION }} \
          -g ${{ env.GROUP }} \
          -w ${{ env.WORKSPACE }} \
          --stream 2>&1 | tee promptflow/llmops-helper/run_info.txt

    - name: Extract run name
      run: |
        RUN=$(python promptflow/llmops-helper/parse_run_output.py promptflow/llmops-helper/run_info.txt 2>/dev/null)
        if [ -z "$RUN" ]; then
          echo "❌ Failed to extract RUN_NAME"
          cat promptflow/llmops-helper/run_info.txt
          exit 1
        fi
        echo "RUN_NAME=$RUN" >> $GITHUB_ENV

    - name: Show run name
      run: echo "Run name is: ${{ env.RUN_NAME }}"

    - name: Show promptflow run results
      run: pfazure run show-details \
        --name ${{ env.RUN_NAME }} \
        --subscription ${{ env.SUBSCRIPTION }} \
        -g ${{ env.GROUP }} \
        -w ${{ env.WORKSPACE }}

    - name: Run evaluation flow
      run: |
        pfazure run create \
          -f promptflow/web-classification/run_evaluation.yml \
          --run ${{ env.RUN_NAME }} \
          --subscription ${{ env.SUBSCRIPTION }} \
          -g ${{ env.GROUP }} \
          -w ${{ env.WORKSPACE }} \
          --stream > promptflow/llmops-helper/eval_info.txt

    - name: Extract evaluation name
      run: |
        EVAL=$(python promptflow/llmops-helper/parse_run_output.py promptflow/llmops-helper/eval_info.txt 2>/dev/null)
        if [ -z "$EVAL" ]; then
          echo "❌ Failed to extract EVAL_RUN_NAME"
          cat promptflow/llmops-helper/eval_info.txt
          exit 1
        fi
        echo "EVAL_RUN_NAME=$EVAL" >> $GITHUB_ENV

    - name: Show evaluation name
      run: echo "Eval name is: ${{ env.EVAL_RUN_NAME }}"

    - name: Show evaluation details
      run: pfazure run show-details \
        --name ${{ env.EVAL_RUN_NAME }} \
        --subscription ${{ env.SUBSCRIPTION }} \
        -g ${{ env.GROUP }} \
        -w ${{ env.WORKSPACE }}

    - name: Export metrics
      run: pfazure run show-metrics \
        --name ${{ env.EVAL_RUN_NAME }} \
        --subscription ${{ env.SUBSCRIPTION }} \
        -g ${{ env.GROUP }} \
        -w ${{ env.WORKSPACE }} \
        > promptflow/llmops-helper/eval_result.json

    - name: Upload evaluation results
      uses: actions/upload-artifact@v3
      with:
        name: eval_metrics
        path: promptflow/llmops-helper/eval_result.json
